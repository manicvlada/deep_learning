{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/manicvlada/deep_learning/blob/main/ML4BI_E3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Machine Learning for BI 2\n",
        "\n",
        "## Deep Learning exercises for week 3"
      ],
      "metadata": {
        "id": "LhmlWPq36NGe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this exercise, you'll practice on the examples we went through during the lectures."
      ],
      "metadata": {
        "id": "R86GoAnuNAul"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Try one or more!\n",
        "\n",
        "1. You used two representation layers before the final classification layer. Try using one or three representation layers, and see how doing so affects validation and test accuracy\n",
        "\n",
        "2. Try using layers with more units or fewer units: 32 units, 64 units, and so on."
      ],
      "metadata": {
        "id": "v7dKmBcUOCM1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Loading the IMDB dataset**"
      ],
      "metadata": {
        "id": "s95akDzCSbJa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fKX6saifMo5c"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.datasets import imdb\n",
        "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(\n",
        "    num_words=10000)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Encoding the integer sequences via multi-hot encoding**"
      ],
      "metadata": {
        "id": "yfMqtfUAUQJo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def vectorize_sequences(sequences, dimension=10000):\n",
        "    results = np.zeros((len(sequences), dimension))\n",
        "    for i, sequence in enumerate(sequences):\n",
        "        for j in sequence:\n",
        "            results[i, j] = 1.\n",
        "    return results\n",
        "x_train = vectorize_sequences(train_data)\n",
        "x_test = vectorize_sequences(test_data)"
      ],
      "metadata": {
        "id": "BLa2zDaFUQgm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = np.asarray(train_labels).astype(\"float32\")\n",
        "y_test = np.asarray(test_labels).astype(\"float32\")"
      ],
      "metadata": {
        "id": "ckbX-s7FUT1v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Building your model"
      ],
      "metadata": {
        "id": "lSoAPMlmUW2n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "model = keras.Sequential([\n",
        "    ## YOUR CODE HERE\n",
        "    layers.Dense(1, activation=\"sigmoid\")\n",
        "])"
      ],
      "metadata": {
        "id": "rsGTgLKyUZRC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer=\"rmsprop\",\n",
        "              loss=\"binary_crossentropy\",\n",
        "              metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "id": "kgoR_a2lUeIG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_val = x_train[:10000]\n",
        "partial_x_train = x_train[10000:]\n",
        "y_val = y_train[:10000]\n",
        "partial_y_train = y_train[10000:]"
      ],
      "metadata": {
        "id": "anTsCN17Uf3Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(partial_x_train,\n",
        "                    partial_y_train,\n",
        "                    epochs=20,\n",
        "                    batch_size=512,\n",
        "                    validation_data=(x_val, y_val),\n",
        "                    verbose=0)"
      ],
      "metadata": {
        "id": "pES45acbUh4m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "history_dict = history.history\n",
        "loss_values = history_dict[\"loss\"]\n",
        "val_loss_values = history_dict[\"val_loss\"]\n",
        "epochs = range(1, len(loss_values) + 1)\n",
        "plt.plot(epochs, loss_values, \"bo\", label=\"Training loss\")\n",
        "plt.plot(epochs, val_loss_values, \"b\", label=\"Validation loss\")\n",
        "plt.title(\"Training and validation loss\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "yavnxHdEUoH1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Newswire\n",
        "\n",
        "### Information bottleneck\n",
        "\n",
        "Try creating an information bottleneck in your architecture. For example, you can this by using 64 neurons in the first layer and 4 neurons in the second."
      ],
      "metadata": {
        "id": "9buo88f1pceh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Try different types of configurations and see how the validation loss or accuracy curve is affected. Write a function that allows to change the number of neurons in each layer and then make a joint plot of all the curves afterwards.\n",
        "\n",
        "I have copied in the data loading and labels encoding steps from the course notebook to help you get started."
      ],
      "metadata": {
        "id": "AjIlKGJ_zGXw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Loading the Reuters dataset**"
      ],
      "metadata": {
        "id": "LgNB9yPHpxT2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.datasets import reuters\n",
        "(train_data, train_labels), (test_data, test_labels) = reuters.load_data(\n",
        "    num_words=10000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d0YuXAD9pp9D",
        "outputId": "1eb8faec-d60f-421e-bbd2-b54a0e4980e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/reuters.npz\n",
            "2110848/2110848 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Decoding newswires back to text**"
      ],
      "metadata": {
        "id": "U-IbS0Zqpzn5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = vectorize_sequences(train_data)\n",
        "x_test = vectorize_sequences(test_data)"
      ],
      "metadata": {
        "id": "dUHd6Mk9pqmA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Encoding the labels**"
      ],
      "metadata": {
        "id": "32W0i45mp28C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def to_one_hot(labels, dimension=46):\n",
        "    results = np.zeros((len(labels), dimension))\n",
        "    for i, label in enumerate(labels):\n",
        "        results[i, label] = 1.\n",
        "    return results\n",
        "y_train = to_one_hot(train_labels)\n",
        "y_test = to_one_hot(test_labels)"
      ],
      "metadata": {
        "id": "v_c6rvCgp3c6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "y_train = to_categorical(train_labels)\n",
        "y_test = to_categorical(test_labels)"
      ],
      "metadata": {
        "id": "fyZj5nvHp6ay"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Setting aside a validation set**"
      ],
      "metadata": {
        "id": "XTmxLsNVqhXt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_val = x_train[:1000]\n",
        "partial_x_train = x_train[1000:]\n",
        "y_val = y_train[:1000]\n",
        "partial_y_train = y_train[1000:]"
      ],
      "metadata": {
        "id": "JEljXdrPqgRD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Building your model"
      ],
      "metadata": {
        "id": "YupIj34PqKg9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##YOUR CODE HERE"
      ],
      "metadata": {
        "id": "BOw-wG4w6lUt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}